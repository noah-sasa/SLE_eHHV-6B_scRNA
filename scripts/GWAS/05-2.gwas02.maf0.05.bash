################
### MAF = 5% ###
################

mkdir data_maf0.05

# Remove SNPs with a low MAF frequency.
# A conventional MAF threshold for a regular GWAS is between 0.01 or 0.05, depending on sample size.
# SNPs with a low MAF are rare, therefore power is lacking for detecting SNP‐phenotype associations. These SNPs are also more prone to genotyping errors. The MAF threshold should depend on your sample size, larger samples can use lower MAF thresholds. Respectively, for large (N = 100.000) vs. moderate samples (N = 10000), 0.01 and 0.05 are commonly used as MAF threshold.
plink --threads 8 --memory 10000\
 --bfile data/joint_22_220_07\
 --maf 0.05\
 --make-bed\
 --out data_maf0.05/joint_22_220_08
##  11905977 variants removed due to minor allele threshold(s)
##  (--maf/--max-maf/--mac/--max-mac).
##  5896014 variants and 242 people pass filters and QC.



####################################################
### Step 4 ###

# Delete SNPs which are not in Hardy-Weinberg equilibrium (HWE).
# Check the distribution of HWE p-values of all SNPs.

plink --threads 8 --memory 10000\
 --bfile data_maf0.05/joint_22_220_08\
 --hardy\
 --out data_maf0.05/joint_22_220_08

# Selecting SNPs with HWE p-value below 0.00001, required for one of the two plot generated by the next Rscript, allows to zoom in on strongly deviating SNPs. 
#awk '{ if ($9 <0.000001) print $0 }' data_maf0.05/joint_22_220_08.hwe > data_maf0.05/joint_22_220_08.zoomhwe.hwe
#awk '{ if ($9 <0.0000000001) print $0 }' data_maf0.05/joint_22_220_08.hwe > data_maf0.05/joint_22_220_08.zoomzoomhwe.hwe
Rscript --no-save script/hwe.08.maf0.05.R  # ggplot2

# By default the --hwe option in plink only filters for controls.
# For binary traits we suggest to exclude: HWE p value <1e?10 in cases and <1e?6 in controls. Less strict case threshold avoids discarding disease‐associated SNPs under selection (see online tutorial at https://github.com/MareesAT/GWA_tutorial/).
# For quantitative traits, we recommend HWE p value <1e‐6.
# Therefore, we use two steps, first we use a stringent HWE threshold for controls, followed by a less stringent threshold for the case data.
plink --threads 8 --memory 10000\
 --bfile data_maf0.05/joint_22_220_08\
 --hwe 1e-6\
 --make-bed\
 --out data_maf0.05/joint_22_220_08_filter_step1
#   --hwe: 305522 variants removed due to Hardy-Weinberg exact test.

# The HWE threshold for the cases filters out only SNPs which deviate extremely from HWE. 
# This second HWE step only focusses on cases because in the controls all SNPs with a HWE p-value < hwe 1e-6 were already removed
plink --threads 8 --memory 10000\
 --bfile data_maf0.05/joint_22_220_08_filter_step1\
 --hwe 1e-10\
 --hwe-all\
 --make-bed\
 --out data_maf0.05/joint_22_220_09
#   --hwe: 14 variants removed due to Hardy-Weinberg exact test.
#   Note: --hwe-all flag deprecated.  Use "--hwe include-nonctrl".




############################################################
### step 5 ###

# Generate a plot of the distribution of the heterozygosity rate of your subjects.
# And remove individuals with a heterozygosity rate deviating more than 3 sd from the mean.

# Checks for heterozygosity are performed on a set of SNPs which are not highly correlated.
# Therefore, to generate a list of non-(highly)correlated SNPs, we exclude high inversion regions (inversion.txt [High LD regions]) and prune the SNPs using the command --indep-pairwise・
# The parameters ・0 5 0.2・stand respectively for: the window size, the number of SNPs to shift the window at each step, and the multiple correlation coefficient for a SNP being regressed on all other SNPs simultaneously.

plink --threads 8 --memory 10000\
 --bfile data_maf0.05/joint_22_220_09\
  --exclude /work23/home/nsasa/data/CHM13v2.0_hhv6/gwas/data/inversion.txt --range\
 --indep-pairwise 50 5 0.2\
 --out data_maf0.05/joint_22_220_09.indepSNP
#   Note: --range flag deprecated.  Use e.g. "--extract range <filename>".
# Note, don't delete the file indepSNP.prune.in, we will use this file in later steps of the tutorial.
#Pruning complete.  5946638 of 6397522 variants removed.

plink --threads 8 --memory 10000\
 --bfile data_maf0.05/joint_22_220_09\
 --extract data_maf0.05/joint_22_220_09.indepSNP.prune.in\
 --het\
 --out data_maf0.05/joint_22_220_09.R_check
# This file contains your pruned data set.

# Plot of the heterozygosity rate distribution
Rscript --no-save script/check_heterozygosity_rate.maf0.05.R

# The following code generates a list of individuals who deviate more than 3 standard deviations from the heterozygosity rate mean.
# For data manipulation we recommend using UNIX. However, when performing statistical calculations R might be more convenient, hence the use of the Rscript for this step:
Rscript --no-save script/heterozygosity_outliers_list.maf0.05.R

# Output of the command above: fail-het-qc.txt .
# When using our example data/the HapMap data this list contains 2 individuals (i.e., two individuals have a heterozygosity rate deviating more than 3 SD's from the mean).
# Adapt this file to make it compatible for PLINK, by removing all quotation marks from the file and selecting only the first two columns.
sed 's/"// g' data_maf0.05/joint_22_220_09.fail-het-qc.txt | awk '{print$1, $2}'> data_maf0.05/joint_22_220_09.het_fail_ind.txt

# Remove heterozygosity rate outliers.
cp data_maf0.05/joint_22_220_09.het_fail_ind.txt data_maf0.05/joint_22_220_09.het_fail_ind.plus.txt
echo -e "SG_SL_UO_00174\tSG_SL_UO_00174" >> data_maf0.05/joint_22_220_09.het_fail_ind.plus.txt
plink --threads 8 --memory 10000\
 --bfile data_maf0.05/joint_22_220_09\
 --remove data_maf0.05/joint_22_220_09.het_fail_ind.plus.txt\
 --make-bed\
 --out data_maf0.05/joint_22_220_10
#   6464509 variants and 238 people pass filters and QC.







############################################################
### step 6 ###

# It is essential to check datasets you analyse for cryptic relatedness.
# Assuming a random population sample we are going to exclude all individuals above the pihat threshold of 0.2 in this tutorial.

# Check for relationships between individuals with a pihat > 0.2.
# --genome --min 0.2\
plink --threads 8 --memory 10000\
 --bfile data_maf0.05/joint_22_220_10\
 --extract data_maf0.05/joint_22_220_09.indepSNP.prune.in\
 --genome\
 --out data_maf0.05/joint_22_220_10.pihat
#   --extract: 450884 variants remaining.

# The HapMap dataset is known to contain parent-offspring relations. 
# The following commands will visualize specifically these parent-offspring relations, using the z values. 
awk '{ if ($8 >0.9) print $0 }' data_maf0.05/joint_22_220_10.pihat.genome > data_maf0.05/joint_22_220_10.zoom_pihat.genome

# Generate a plot to assess the type of relationship.
Rscript --no-save script/Relatedness.maf0.05.R

# The generated plots show a considerable amount of related individuals (explentation plot; PO = parent-offspring, UN = unrelated individuals) in the Hapmap data, this is expected since the dataset was constructed as such.
# Normally, family based data should be analyzed using specific family based methods. In this tutorial, for demonstrative purposes, we treat the relatedness as cryptic relatedness in a random population sample.
# In this tutorial, we aim to remove all 'relatedness' from our dataset.
# To demonstrate that the majority of the relatedness was due to parent-offspring we only include founders (individuals without parents in the dataset).

plink --threads 8 --memory 10000\
 --bfile data_maf0.05/joint_22_220_10\
 --filter-founders\
 --make-bed\
 --out data_maf0.05/joint_22_220_11
#   6464509 variants and 238 people pass filters and QC.

# Now we will look again for individuals with a pihat >0.2.
plink --threads 8 --memory 10000\
 --bfile data_maf0.05/joint_22_220_11\
 --extract data_maf0.05/joint_22_220_09.indepSNP.prune.in --genome --min 0.2 --out data_maf0.05/joint_22_220_11.pihat_min0.2_in_founders
# The file 'pihat_min0.2_in_founders.genome' shows that, after exclusion of all non-founders, only 1 individual pair with a pihat greater than 0.2 remains in the HapMap data.
# This is likely to be a full sib or DZ twin pair based on the Z values. Noteworthy, they were not given the same family identity (FID) in the HapMap data.


#   ## QC on HapMap3 data.
#   # Remove variants based on missing genotype data.
#   plink --threads 8 --memory 10000\
#    --file hapmap3/hapmap3_r2_b36_fwd.consensus.qc.poly\
#    --geno 0.2\
#    --allow-no-sex\
#    --make-bed\
#    --out hapmap3/hapmap_3_r2_b36_MDS
#   #   0 variants removed due to missing genotype data (--geno).
#   
#   # Remove individuals based on missing genotype data.
#   plink --threads 8 --memory 10000\
#    --bfile hapmap3/hapmap_3_r2_b36_MDS\
#    --mind 0.2\
#    --allow-no-sex\
#    --make-bed\
#    --out hapmap3/hapmap_3_r2_b36_MDS2
#   #   0 people removed due to missing genotype data (--mind).
#   
#   # Remove variants based on missing genotype data.
#   plink --threads 8 --memory 10000\
#    --bfile hapmap3/hapmap_3_r2_b36_MDS2\
#    --geno 0.02\
#    --allow-no-sex\
#    --make-bed\
#    --out hapmap3/hapmap_3_r2_b36_MDS3
#   #   4524 variants removed due to missing genotype data (--geno).
#   
#   # Remove individuals based on missing genotype data.
#   plink --threads 8 --memory 10000\
#    --bfile hapmap3/hapmap_3_r2_b36_MDS3\
#    --mind 0.02\
#    --allow-no-sex\
#    --make-bed\
#    --out hapmap3/hapmap_3_r2_b36_MDS4
#   #   4 people removed due to missing genotype data (--mind).
#   
#   # Remove variants based on MAF.
#   plink --threads 8 --memory 10000\
#    --bfile hapmap3/hapmap_3_r2_b36_MDS4\
#    --maf 0.05\
#    --allow-no-sex\
#    --make-bed\
#    --out hapmap3/hapmap_3_r2_b36_MDS5
#   #   191590 variants removed due to minor allele threshold(s)
#   #   1244502 variants and 1180 people pass filters and QC.
#   
#   rm hapmap3/hapmap_3_r2_b36_MDS4.bed hapmap3/hapmap_3_r2_b36_MDS4.bim hapmap3/hapmap_3_r2_b36_MDS4.fam
#   rm hapmap3/hapmap_3_r2_b36_MDS3.bed hapmap3/hapmap_3_r2_b36_MDS3.bim hapmap3/hapmap_3_r2_b36_MDS3.fam
#   rm hapmap3/hapmap_3_r2_b36_MDS2.bed hapmap3/hapmap_3_r2_b36_MDS2.bim hapmap3/hapmap_3_r2_b36_MDS2.fam
#   rm hapmap3/hapmap_3_r2_b36_MDS.bed hapmap3/hapmap_3_r2_b36_MDS.bim hapmap3/hapmap_3_r2_b36_MDS.fam





##############################################################
############### START ANALISIS ###############################
##############################################################

### data02
mkdir -p data02_maf0.05

# Extract the variants present in my dataset from the HapMap3 dataset.
awk '{print$2}' data_maf0.05/joint_22_220_11.bim > data02_maf0.05/joint_22_220_11_SNPs.txt
plink --threads 8 --memory 10000\
 --bfile ../gwas/hapmap3/hapmap_3_r2_b36_MDS5\
 --extract data02_maf0.05/joint_22_220_11_SNPs.txt\
 --make-bed\
 --out data02_maf0.05/hapmap_3_r2_b36_MDS6
#   --extract: 948291 variants remaining.
#   948291 variants and 1180 people pass filters and QC.

# Extract the variants present in the HapMap3 dataset from my dataset.
awk '{print$2}' data02_maf0.05/hapmap_3_r2_b36_MDS6.bim > data02_maf0.05/hapmap_3_r2_b36_MDS6_SNPs.txt
plink --threads 8 --memory 10000\
 --bfile data_maf0.05/joint_22_220_11\
 --extract data02_maf0.05/hapmap_3_r2_b36_MDS6_SNPs.txt\
 --recode --make-bed\
 --out data02_maf0.05/joint_22_220_MDS
#   --extract: 948291 variants remaining.
#   948291 variants and 238 people pass filters and QC.
# The datasets now contain the exact same variants.

## The datasets must have the same build. Change the build 1000 Genomes data build.
awk '{print$2,$4}' data02_maf0.05/joint_22_220_MDS.map > data02_maf0.05/joint_22_220_MDS_buildhapmap.txt
# buildhapmap.txt contains one SNP-id and physical position per line.

plink --threads 8 --memory 10000\
 --bfile data02_maf0.05/hapmap_3_r2_b36_MDS6\
 --update-map data02_maf0.05/joint_22_220_MDS_buildhapmap.txt\
 --make-bed\
 --out data02_maf0.05/hapmap_3_r2_b36_MDS7
# hapmap_3_r2_b36_MDS7 and joint_22_220_MDS now have the same build.




## Merge mydata and the HapMap data sets

# Prior to merging 1000 Genomes data with the HapMap data we want to make sure that the files are mergeable, for this we conduct 3 steps:
# 1) Make sure the reference genome is similar in the HapMap and the 1000 Genomes Project datasets.
# 2) Resolve strand issues.
# 3) Remove the SNPs which after the previous two steps still differ between datasets.

# The following steps are maybe quite technical in terms of commands, but we just compare the two data sets and make sure they correspond.

# 1) set reference genome 
awk '{print$2,$5}' data02_maf0.05/hapmap_3_r2_b36_MDS7.bim > data02_maf0.05/hapmap_3_r2_b36_MDS7_ref-list.txt
plink --threads 8 --memory 10000\
 --bfile data02_maf0.05/joint_22_220_MDS\
 --reference-allele data02_maf0.05/hapmap_3_r2_b36_MDS7_ref-list.txt\
 --make-bed\
 --out data02_maf0.05/joint_22_220-adj
# The 1kG_MDS7 and the HapMap-adj have the same reference genome for all SNPs.
# This command will generate some warnings for impossible A1 allele assignment.

# 2) Resolve strand issues.
# Check for potential strand issues.
awk '{print$2,$5,$6}' data02_maf0.05/hapmap_3_r2_b36_MDS7.bim > data02_maf0.05/hapmap_3_r2_b36_MDS7_tmp
awk '{print$2,$5,$6}' data02_maf0.05/joint_22_220-adj.bim > data02_maf0.05/joint_22_220-adj_tmp
sort data02_maf0.05/hapmap_3_r2_b36_MDS7_tmp data02_maf0.05/joint_22_220-adj_tmp |uniq -u > data02_maf0.05/all_differences.txt
# 6958 differences between the files, some of these might be due to strand issues.

## Flip SNPs for resolving strand issues.
# Print SNP-identifier and remove duplicates.
awk '{print$1}' data02_maf0.05/all_differences.txt | sort -u > data02_maf0.05/flip_list.txt
# Generates a file of 3479 SNPs. These are the non-corresponding SNPs between the two files. 
# Flip the 3479 non-corresponding SNPs. 
plink --threads 8 --memory 10000\
 --bfile data02_maf0.05/joint_22_220-adj\
 --flip data02_maf0.05/flip_list.txt\
 --reference-allele data02_maf0.05/hapmap_3_r2_b36_MDS7_ref-list.txt\
 --make-bed\
 --out data02_maf0.05/joint_22_220-corrected

# Check for SNPs which are still problematic after they have been flipped.
awk '{print$2,$5,$6}' data02_maf0.05/joint_22_220-corrected.bim > data02_maf0.05/joint_22_220-corrected_tmp
sort data02_maf0.05/hapmap_3_r2_b36_MDS7_tmp data02_maf0.05/joint_22_220-corrected_tmp |uniq -u  > data02_maf0.05/uncorresponding_SNPs.txt
# This file demonstrates that there are 112 differences between the files.

# 3) Remove problematic SNPs from HapMap and 1000 Genomes.
awk '{print$1}' data02_maf0.05/uncorresponding_SNPs.txt | sort -u > data02_maf0.05/SNPs_for_exlusion.txt
# The command above generates a list of the 56 SNPs which caused the 112 differences between the HapMap and the 1000 Genomes data sets after flipping and setting of the reference genome.

# Remove the 56 problematic SNPs from both datasets.
plink --threads 8 --memory 10000\
 --bfile data02_maf0.05/joint_22_220-corrected\
 --exclude data02_maf0.05/SNPs_for_exlusion.txt\
 --make-bed\
 --out data02_maf0.05/joint_22_220_MDS2
#   --exclude: 948235 variants remaining.
plink --threads 8 --memory 10000\
 --bfile data02_maf0.05/hapmap_3_r2_b36_MDS7\
 --exclude data02_maf0.05/SNPs_for_exlusion.txt\
 --make-bed\
 --out data02_maf0.05/hapmap_3_r2_b36_MDS8
#   --exclude: 948235 variants remaining.

# Merge HapMap with 1000 Genomes Data.
plink --threads 8 --memory 10000\
 --bfile data02_maf0.05/joint_22_220_MDS2\
 --bmerge data02_maf0.05/hapmap_3_r2_b36_MDS8.bed data02_maf0.05/hapmap_3_r2_b36_MDS8.bim data02_maf0.05/hapmap_3_r2_b36_MDS8.fam\
 --allow-no-sex\
 --make-bed\
 --out data02_maf0.05/MDS_merge2
#   948235 variants and 1418 people pass filters and QC.

# Note, we are fully aware of the sample overlap between the HapMap and 1000 Genomes datasets. However, for the purpose of this tutorial this is not important.

## Perform MDS on HapMap-CEU data anchored by 1000 Genomes data.
# Using a set of pruned SNPs
plink --threads 8 --memory 10000\
 --bfile data02_maf0.05/MDS_merge2\
 --extract data_maf0.05/joint_22_220_09.indepSNP.prune.in\
 --genome\
 --out data02_maf0.05/MDS_merge2
plink --threads 8 --memory 10000\
 --bfile data02_maf0.05/MDS_merge2\
 --read-genome data02_maf0.05/MDS_merge2.genome\
 --cluster\
 --mds-plot 10\
 --out data02_maf0.05/MDS_merge2




### MDS-plot

#   # Download the file with population information of the 1000 genomes dataset.
#   wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20100804/20100804.ALL.panel
#   # The file 20100804.ALL.panel contains population codes of the individuals of 1000 genomes.
#   
#   # Convert population codes into superpopulation codes (i.e., AFR,AMR,ASN, and EUR).
#   awk '{print$1,$1,$2}' 20100804.ALL.panel > race_1kG.txt
#   sed 's/JPT/ASN/g' race_1kG.txt>race_1kG2.txt
#   sed 's/ASW/AFR/g' race_1kG2.txt>race_1kG3.txt
#   sed 's/CEU/EUR/g' race_1kG3.txt>race_1kG4.txt
#   sed 's/CHB/ASN/g' race_1kG4.txt>race_1kG5.txt
#   sed 's/CHD/ASN/g' race_1kG5.txt>race_1kG6.txt
#   sed 's/YRI/AFR/g' race_1kG6.txt>race_1kG7.txt
#   sed 's/LWK/AFR/g' race_1kG7.txt>race_1kG8.txt
#   sed 's/TSI/EUR/g' race_1kG8.txt>race_1kG9.txt
#   sed 's/MXL/AMR/g' race_1kG9.txt>race_1kG10.txt
#   sed 's/GBR/EUR/g' race_1kG10.txt>race_1kG11.txt
#   sed 's/FIN/EUR/g' race_1kG11.txt>race_1kG12.txt
#   sed 's/CHS/ASN/g' race_1kG12.txt>race_1kG13.txt
#   sed 's/PUR/AMR/g' race_1kG13.txt>race_1kG14.txt
#   
#   # Create a racefile of your own data.
#   awk '{print$1,$2,"OWN"}' HapMap_MDS.fam>racefile_own.txt
#   
#   # Concatenate racefiles.
#   cat race_1kG14.txt racefile_own.txt | sed -e '1i\FID IID race' > racefile.txt

cat data02_maf0.05/joint_22_220_MDS.fam | awk '{print$1,$2,"OWN"}' > data02_maf0.05/racefile_own.txt
tail -n +2 ../gwas/hapmap3/pops_HapMap_3_r2 | awk '{print $1,$2,$7}' > data02_maf0.05/racefile_hapmap3.txt
echo -e "FID IID population" > data02_maf0.05/racefile.txt
cat data02_maf0.05/racefile_hapmap3.txt data02_maf0.05/racefile_own.txt >> data02_maf0.05/racefile.txt

# Generate population stratification plot.
source /work23/home/nsasa/conda-pack/r-ggplot2/bin/activate
Rscript script/MDS_merged.maf0.05.R 


source /work23/home/nsasa/conda-pack/r-adegenet/bin/activate
# The output file MDS.pdf demonstrates that our 双wn・data falls within the European group of the 1000 genomes data. Therefore, we do not have to remove subjects.
# For educational purposes however, we give scripts below to filter out population stratification outliers. Please execute the script below in order to generate the appropriate files for the next tutorial.

## Exclude ethnic outliers.
# Select individuals in HapMap data below cut-off thresholds. The cut-off levels are not fixed thresholds but have to be determined based on the visualization of the first two dimensions. To exclude ethnic outliers, the thresholds need to be set around the cluster of population of interest.
awk '{ if ($4 <-0.05 && $5 < -0.03) print $1,$2 }' data02_maf0.05/MDS_merge2.mds > data02_maf0.05/ASN_MDS_merge2

# Extract these individuals in HapMap data.
plink --threads 8 --memory 10000\
 --bfile data_maf0.05/joint_22_220_11\
 --keep data02_maf0.05/ASN_MDS_merge2\
 --make-bed\
 --out data02_maf0.05/joint_22_220_13
#   --keep: 238 people remaining.
#   6464509 variants and 238 people pass filters and QC.
# Note, since our HapMap data did include any ethnic outliers, no individuls were removed at this step. However, if our data would have included individuals outside of the thresholds we set, then these individuals would have been removed.

## Create covariates based on MDS.
# Perform an MDS ONLY on HapMap data without ethnic outliers. The values of the 10 MDS dimensions are subsequently used as covariates in the association analysis in the third tutorial.
plink --threads 8 --memory 10000\
 --bfile data02_maf0.05/joint_22_220_13\
 --extract data_maf0.05/joint_22_220_09.indepSNP.prune.in\
 --genome\
 --out data02_maf0.05/joint_22_220_13
plink --threads 8 --memory 10000\
 --bfile data02_maf0.05/joint_22_220_13\
 --read-genome data02_maf0.05/joint_22_220_13.genome\
 --cluster\
 --mds-plot 10\
 --out data02_maf0.05/joint_22_220_13_mds

# Change the format of the .mds file into a plink covariate file.
awk '{print$1, $2, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13}' data02_maf0.05/joint_22_220_13_mds.mds > data02_maf0.05/covar_mds.txt

## Create covariates based on PCA.
plink --threads 8 --memory 10000\
 --bfile data02_maf0.05/joint_22_220_13\
 --read-genome data02_maf0.05/joint_22_220_13.genome\
 --cluster\
 --pca 20\
 --out data02_maf0.05/joint_22_220_13_pca

# Change the format of the .mds file into a plink covariate file.
echo -e "FID IID PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 PC12 PC13 PC14 PC15 PC16 PC17 PC18 PC19 PC20" > data02_maf0.05/covar_pca.txt
cat data02_maf0.05/joint_22_220_13_pca.eigenvec >> data02_maf0.05/covar_pca.txt

echo -e "FID IID sex PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 PC12 PC13 PC14 PC15 PC16 PC17 PC18 PC19 PC20" > data02_maf0.05/covar_pca.sex.txt
for SampleID in $(cat data02_maf0.05/covar_pca.txt | awk '{print $1}')
do
  sex=$(tail -n +2 original/pheno.modsex.txt | awk -v SampleID=${SampleID} '$1==SampleID{print $3}')
  tail -n +2 data02_maf0.05/covar_pca.txt | awk -v sex=${sex} -v SampleID=${SampleID} '$1==SampleID{print $1,$2,sex,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$20,$21,$22}' >> data02_maf0.05/covar_pca.sex.txt
done

# 下記ではEXCLUDEした症例も入ってしまうためERROR
#paste -d ' ' <(cat original/pheno.modsex.txt | awk '{print $1,$2,$3}') <(cat data02_maf0.05/covar_pca.txt | awk '{print $3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$20,$21,$22}') > data02_maf0.05/covar_pca.sex.txt

# The values in covar_mds.txt will be used as covariates, to adjust for remaining population stratification, in the third tutorial where we will perform a genome-wide association analysis.

##########################################################################################################################################################################

## CONGRATULATIONS you have succesfully controlled your data for population stratification!

# For the next tutorial you need the following files:
# - data02/joint_22_220_13 (the bfile, i.e., data02/joint_22_220_13.bed,data02/joint_22_220_13.bim,and data02/joint_22_220_13.fam
# - covar_mds.txt







###########################################################
### Association analyses ###

# For the association analyses we use the files generated in the previous tutorial (population stratification), named: HapMap_3_r3_13 (with .bed, .bim, and .fam. extensions) and covar_mds.txt


mkdir -p data03_maf0.05


for i in $(seq 1 20)
do
  plink2 --threads 8 --memory 5000\
   --bfile data02_maf0.05/joint_22_220_13\
   --logistic\
   --covar data02_maf0.05/covar_pca.sex.txt\
   --covar-name sex-PC${i}\
   --out data03_maf0.05/plink2_logistic_results_sex_PC1_PC${i}
done
##  Warning: --glm remaining case count is less than 10x predictor count for phenotype 'PHENO1'.

plink2 --threads 8 --memory 5000\
   --bfile data02_maf0.05/joint_22_220_13\
   --logistic\
   --covar data02_maf0.05/covar_pca.sex.txt\
   --covar-name sex\
   --out data03_maf0.05/plink2_logistic_results_sex

plink2 --threads 8 --memory 10000\
   --bfile data02_maf0.05/joint_22_220_13\
   --logistic dominant\
   --covar data02_maf0.05/covar_pca.sex.txt\
   --covar-name sex\
   --out data03_maf0.05/plink2_logistic_results_dominant_sex

plink2 --threads 8 --memory 10000\
   --bfile data02_maf0.05/joint_22_220_13\
   --logistic allow-no-covars\
   --out data03_maf0.05/plink2_logistic_results_noCov

plink2 --threads 8 --memory 10000\
   --bfile data02_maf0.05/joint_22_220_13\
   --logistic dominant allow-no-covars\
   --out data03_maf0.05/plink2_logistic_results_dominant_noCov

plink2 --threads 8 --memory 5000\
   --bfile data02_maf0.05/joint_22_220_13\
   --logistic dominant\
   --covar data02_maf0.05/covar_pca.sex.txt\
   --covar-name sex-PC2\
   --out data03_maf0.05/plink2_logistic_results_dominant_sex_PC1_PC2

plink2 --threads 8 --memory 5000\
   --bfile data02_maf0.05/joint_22_220_13\
   --logistic\
   --covar data02_maf0.05/covar_pca.sex.txt\
   --covar-name sex-PC2\
   --out data03_maf0.05/plink2_logistic_results_sex_PC1_PC2

plink2 --threads 8 --memory 5000\
   --bfile data02_maf0.05/joint_22_220_13\
   --logistic dominant\
   --covar data02_maf0.05/covar_pca.sex.txt\
   --covar-name sex-PC1\
   --out data03_maf0.05/plink2_logistic_results_dominant_sex_PC1

plink2 --threads 8 --memory 5000\
   --bfile data02_maf0.05/joint_22_220_13\
   --logistic\
   --covar data02_maf0.05/covar_pca.sex.txt\
   --covar-name sex-PC1\
   --out data03_maf0.05/plink2_logistic_results_sex_PC1

for i in $(seq 1 20)
do
  plink2 --threads 8 --memory 100000\
   --bfile data02_maf0.05/joint_22_220_13\
   --logistic dominant\
   --covar data02_maf0.05/covar_pca.sex.txt\
   --covar-name sex-PC${i}\
   --out data03_maf0.05/plink2_logistic_results_dominant_sex_PC1_PC${i}
done


source /work23/home/nsasa/conda-pack/r-qqman/bin/activate

echo -e "CHROM\tPOS\tID\tA1\tA2\tOR\tLOG(OR)_SE\tZ_STAT\tP\tERRCODE" > data03_maf0.05/plink2_logistic_results_noCov.PHENO1.glm.logistic.ADDonly.removeNA.tsv
tail -n +2 data03_maf0.05/plink2_logistic_results_noCov.PHENO1.glm.logistic.hybrid | awk '$8=="ADD" && $13!="NA"{print $1"\t"$2"\t"$3"\t"$5"\t"$4"\t"$10"\t"$11"\t"$12"\t"$13"\t"$14}' >> data03_maf0.05/plink2_logistic_results_noCov.PHENO1.glm.logistic.ADDonly.removeNA.tsv
source /work23/home/nsasa/conda-pack/r-qqman/bin/activate
Rscript script/QQ_plot.args.maf0.05.R noCov
Rscript script/Manhattan_plot.args.maf0.05.R noCov

echo -e "CHROM\tPOS\tID\tA1\tA2\tOR\tLOG(OR)_SE\tZ_STAT\tP\tERRCODE" > data03_maf0.05/plink2_logistic_results_noCov.PHENO1.glm.logistic.DOMonly.removeNA.tsv
tail -n +2 data03_maf0.05/plink2_logistic_results_dominant_noCov.PHENO1.glm.logistic.hybrid | awk '$8=="DOM" && $13!="NA"{print $1"\t"$2"\t"$3"\t"$5"\t"$4"\t"$10"\t"$11"\t"$12"\t"$13"\t"$14}' >> data03_maf0.05/plink2_logistic_results_noCov.PHENO1.glm.logistic.DOMonly.removeNA.tsv
source /work23/home/nsasa/conda-pack/r-qqman/bin/activate
Rscript script/QQ_plot.DOM.args.maf0.05.R noCov
Rscript script/Manhattan_plot.DOM.args.maf0.05.R noCov

echo -e "CHROM\tPOS\tID\tA1\tA2\tOR\tLOG(OR)_SE\tZ_STAT\tP\tERRCODE" > data03_maf0.05/plink2_logistic_results_sex_PC1_PC2.PHENO1.glm.logistic.ADDonly.removeNA.tsv
tail -n +2 data03_maf0.05/plink2_logistic_results_sex_PC1_PC2.PHENO1.glm.logistic.hybrid | awk '$8=="ADD" && $13!="NA"{print $1"\t"$2"\t"$3"\t"$5"\t"$4"\t"$10"\t"$11"\t"$12"\t"$13"\t"$14}' >> data03_maf0.05/plink2_logistic_results_sex_PC1_PC2.PHENO1.glm.logistic.ADDonly.removeNA.tsv
source /work23/home/nsasa/conda-pack/r-qqman/bin/activate
Rscript script/QQ_plot.args.maf0.05.R sex_PC1_PC2
Rscript script/Manhattan_plot.args.maf0.05.R sex_PC1_PC2

echo -e "CHROM\tPOS\tID\tA1\tA2\tOR\tLOG(OR)_SE\tZ_STAT\tP\tERRCODE" > data03_maf0.05/plink2_logistic_results_sex_PC1_PC2.PHENO1.glm.logistic.DOMonly.removeNA.tsv
tail -n +2 data03_maf0.05/plink2_logistic_results_dominant_sex_PC1_PC2.PHENO1.glm.logistic.hybrid | awk '$8=="DOM" && $13!="NA"{print $1"\t"$2"\t"$3"\t"$5"\t"$4"\t"$10"\t"$11"\t"$12"\t"$13"\t"$14}' >> data03_maf0.05/plink2_logistic_results_sex_PC1_PC2.PHENO1.glm.logistic.DOMonly.removeNA.tsv
source /work23/home/nsasa/conda-pack/r-qqman/bin/activate
Rscript script/QQ_plot.DOM.args.maf0.05.R sex_PC1_PC2
Rscript script/Manhattan_plot.DOM.args.maf0.05.R sex_PC1_PC2

echo -e "CHROM\tPOS\tID\tA1\tA2\tOR\tLOG(OR)_SE\tZ_STAT\tP\tERRCODE" > data03_maf0.05/plink2_logistic_results_sex_PC1.PHENO1.glm.logistic.ADDonly.removeNA.tsv
tail -n +2 data03_maf0.05/plink2_logistic_results_sex_PC1.PHENO1.glm.logistic.hybrid | awk '$8=="ADD" && $13!="NA"{print $1"\t"$2"\t"$3"\t"$5"\t"$4"\t"$10"\t"$11"\t"$12"\t"$13"\t"$14}' >> data03_maf0.05/plink2_logistic_results_sex_PC1.PHENO1.glm.logistic.ADDonly.removeNA.tsv
source /work23/home/nsasa/conda-pack/r-qqman/bin/activate
Rscript script/QQ_plot.args.maf0.05.R sex_PC1
Rscript script/Manhattan_plot.args.maf0.05.R sex_PC1

echo -e "CHROM\tPOS\tID\tA1\tA2\tOR\tLOG(OR)_SE\tZ_STAT\tP\tERRCODE" > data03_maf0.05/plink2_logistic_results_sex_PC1.PHENO1.glm.logistic.DOMonly.removeNA.tsv
tail -n +2 data03_maf0.05/plink2_logistic_results_dominant_sex_PC1.PHENO1.glm.logistic.hybrid | awk '$8=="DOM" && $13!="NA"{print $1"\t"$2"\t"$3"\t"$5"\t"$4"\t"$10"\t"$11"\t"$12"\t"$13"\t"$14}' >> data03_maf0.05/plink2_logistic_results_sex_PC1.PHENO1.glm.logistic.DOMonly.removeNA.tsv
source /work23/home/nsasa/conda-pack/r-qqman/bin/activate
Rscript script/QQ_plot.DOM.args.maf0.05.R sex_PC1
Rscript script/Manhattan_plot.DOM.args.maf0.05.R sex_PC1

echo -e "CHROM\tPOS\tID\tA1\tA2\tOR\tLOG(OR)_SE\tZ_STAT\tP\tERRCODE" > data03_maf0.05/plink2_logistic_results_sex.PHENO1.glm.logistic.ADDonly.removeNA.tsv
tail -n +2 data03_maf0.05/plink2_logistic_results_sex.PHENO1.glm.logistic.hybrid | awk '$8=="ADD" && $13!="NA"{print $1"\t"$2"\t"$3"\t"$5"\t"$4"\t"$10"\t"$11"\t"$12"\t"$13"\t"$14}' >> data03_maf0.05/plink2_logistic_results_sex.PHENO1.glm.logistic.ADDonly.removeNA.tsv
source /work23/home/nsasa/conda-pack/r-qqman/bin/activate
Rscript script/QQ_plot.args.maf0.05.R sex
Rscript script/Manhattan_plot.args.maf0.05.R sex

source /work23/home/nsasa/conda-pack/r-qqman/bin/activate
for i in $(seq 1 20)
do
  echo -e "CHROM\tPOS\tID\tA1\tA2\tOR\tLOG(OR)_SE\tZ_STAT\tP\tERRCODE" > data03_maf0.05/plink2_logistic_results_sex_PC1_PC${i}.PHENO1.glm.logistic.ADDonly.removeNA.tsv
  tail -n +2 data03_maf0.05/plink2_logistic_results_sex_PC1_PC${i}.PHENO1.glm.logistic.hybrid | awk '$8=="ADD" && $13!="NA"{print $1"\t"$2"\t"$3"\t"$5"\t"$4"\t"$10"\t"$11"\t"$12"\t"$13"\t"$14}' >> data03_maf0.05/plink2_logistic_results_sex_PC1_PC${i}.PHENO1.glm.logistic.ADDonly.removeNA.tsv
  covariate="sex_PC1_PC"${i}
  Rscript script/QQ_plot.args.maf0.05.R ${covariate}
  Rscript script/Manhattan_plot.args.maf0.05.R ${covariate}
done

echo -e "CHROM\tPOS\tID\tA1\tA2\tOR\tLOG(OR)_SE\tZ_STAT\tP\tERRCODE" > data03_maf0.05/plink2_logistic_results_sex.PHENO1.glm.logistic.DOMonly.removeNA.tsv
tail -n +2 data03_maf0.05/plink2_logistic_results_dominant_sex.PHENO1.glm.logistic.hybrid | awk '$8=="DOM" && $13!="NA"{print $1"\t"$2"\t"$3"\t"$5"\t"$4"\t"$10"\t"$11"\t"$12"\t"$13"\t"$14}' >> data03_maf0.05/plink2_logistic_results_sex.PHENO1.glm.logistic.DOMonly.removeNA.tsv

for i in $(seq 1 20)
do
  echo -e "CHROM\tPOS\tID\tA1\tA2\tOR\tLOG(OR)_SE\tZ_STAT\tP\tERRCODE" > data03_maf0.05/plink2_logistic_results_sex_PC1_PC${i}.PHENO1.glm.logistic.DOMonly.removeNA.tsv
  tail -n +2 data03_maf0.05/plink2_logistic_results_dominant_sex_PC1_PC${i}.PHENO1.glm.logistic.hybrid | awk '$8=="DOM" && $13!="NA"{print $1"\t"$2"\t"$3"\t"$5"\t"$4"\t"$10"\t"$11"\t"$12"\t"$13"\t"$14}' >> data03_maf0.05/plink2_logistic_results_sex_PC1_PC${i}.PHENO1.glm.logistic.DOMonly.removeNA.tsv
done



### without sex
source /work23/home/nsasa/conda-pack/r-adegenet/bin/activate
plink2 --threads 8 --memory 5000\
   --bfile data02_maf0.05/joint_22_220_13\
   --logistic\
   --covar data02_maf0.05/covar_pca.sex.txt\
   --covar-name PC1\
   --out data03_maf0.05/plink2_logistic_results_PC1

for i in $(seq 2 20)
do
  plink2 --threads 8 --memory 5000\
   --bfile data02_maf0.05/joint_22_220_13\
   --logistic\
   --covar data02_maf0.05/covar_pca.sex.txt\
   --covar-name PC1-PC${i}\
   --out data03_maf0.05/plink2_logistic_results_PC1_PC${i}
done

source /work23/home/nsasa/conda-pack/r-qqman/bin/activate
echo -e "CHROM\tPOS\tID\tA1\tA2\tOR\tLOG(OR)_SE\tZ_STAT\tP\tERRCODE" > data03_maf0.05/plink2_logistic_results_PC1.PHENO1.glm.logistic.ADDonly.removeNA.tsv
tail -n +2 data03_maf0.05/plink2_logistic_results_PC1.PHENO1.glm.logistic.hybrid | awk '$8=="ADD" && $13!="NA"{print $1"\t"$2"\t"$3"\t"$5"\t"$4"\t"$10"\t"$11"\t"$12"\t"$13"\t"$14}' >> data03_maf0.05/plink2_logistic_results_PC1.PHENO1.glm.logistic.ADDonly.removeNA.tsv
covariate="PC1"
Rscript script/QQ_plot.args.maf0.05.R ${covariate}
Rscript script/Manhattan_plot.args.maf0.05.R ${covariate}
for i in $(seq 2 20)
do
  echo -e "CHROM\tPOS\tID\tA1\tA2\tOR\tLOG(OR)_SE\tZ_STAT\tP\tERRCODE" > data03_maf0.05/plink2_logistic_results_PC1_PC${i}.PHENO1.glm.logistic.ADDonly.removeNA.tsv
  tail -n +2 data03_maf0.05/plink2_logistic_results_PC1_PC${i}.PHENO1.glm.logistic.hybrid | awk '$8=="ADD" && $13!="NA"{print $1"\t"$2"\t"$3"\t"$5"\t"$4"\t"$10"\t"$11"\t"$12"\t"$13"\t"$14}' >> data03_maf0.05/plink2_logistic_results_PC1_PC${i}.PHENO1.glm.logistic.ADDonly.removeNA.tsv
  covariate="PC1_PC"${i}
  Rscript script/QQ_plot.args.maf0.05.R ${covariate}
  Rscript script/Manhattan_plot.args.maf0.05.R ${covariate}
done


### ggplot2 + ggrastr
### https://sites.google.com/view/stuck-in-the-shallow-end/home/generate-manhattan-plots-with-ggplot2-and-ggrastr
### https://slowkow.com/notes/ggplot2-qqplot/


Rscript script/ggplot2_manhattan_qq_histogram.R 0.05 sex_PC1_PC2 ADD
Rscript script/ggplot2_manhattan_qq_histogram.R 0.05 sex_PC1_PC2 DOM
Rscript script/ggplot2_manhattan_qq_histogram.R 0.05 sex_PC1 ADD
Rscript script/ggplot2_manhattan_qq_histogram.R 0.05 sex_PC1 DOM
Rscript script/ggplot2_manhattan_qq_histogram.R 0.05 noCov ADD
Rscript script/ggplot2_manhattan_qq_histogram.R 0.05 noCov DOM

Rscript script/ggplot2_manhattan_qq_histogram.R 0.05 sex DOM

for i in $(seq 1 20)
do
    Rscript script/ggplot2_manhattan_qq_histogram.R 0.05 sex_PC1_PC${i} DOM
done
